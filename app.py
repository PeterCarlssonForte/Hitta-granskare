import streamlit as st
import requests
import pandas as pd
import datetime
import time
from sentence_transformers import SentenceTransformer, util
from keybert import KeyBERT
import numpy as np
from typing import List, Dict, Tuple

# ---------- Initiera modeller p√• CPU ----------
@st.cache_resource
def load_models():
    embed_model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
    kw_model = KeyBERT(model=embed_model)
    return embed_model, kw_model

embed_model, kw_model = load_models()

st.title("üîé Hitta forskare f√∂r utlysning (F√∂rb√§ttrad)")
st.caption("Nu med semantisk matchning baserad p√• forskarnas publikationer!")


# ---------- Inputs ----------
call_text = st.text_area("Klistra in utlysningstext h√§r:")

# Nytt: Egna s√∂kord
custom_keywords_str = st.text_input(
    "L√§gg till egna s√∂kord (kommaseparerade, t.ex. arbetsmilj√∂, stresshantering, digitalisering)",
    value=""
)
custom_keywords = [kw.strip() for kw in custom_keywords_str.split(",") if kw.strip()]

num_keywords = st.slider("Hur m√•nga s√∂kord vill du extrahera?", 5, 30, 10)
ngram_range = st.slider("Max l√§ngd p√• fraser (antal ord)", 1, 4, 2)
num_per_source = st.slider("Hur m√•nga forskare per s√∂kord/√§mne?", 5, 50, 20)

# Nya viktningsinst√§llningar
st.sidebar.header("üéØ Matchningsviktning")
semantic_weight = st.sidebar.slider("Semantisk likhet (abstract-matchning)", 0.0, 1.0, 0.5, 0.1)
keyword_weight = st.sidebar.slider("Antal matchade s√∂kord", 0.0, 1.0, 0.3, 0.1)
impact_weight = st.sidebar.slider("Forskningsimpakt (citeringar)", 0.0, 1.0, 0.2, 0.1)

# Normalisera vikter s√• de summerar till 1.0
total_weight = semantic_weight + keyword_weight + impact_weight
if total_weight > 0:
    semantic_weight /= total_weight
    keyword_weight /= total_weight
    impact_weight /= total_weight

include_countries = st.multiselect(
    "Inkludera endast dessa l√§nder (frivilligt, filtreras efter h√§mtning):",
    options=["SE", "NO", "DK", "FI", "US", "DE", "UK", "FR", "NL", "IT"]
)

exclude_countries = st.multiselect(
    "Exkludera forskare fr√•n dessa l√§nder:",
    options=["SE", "NO", "DK", "FI", "US", "DE", "UK", "FR", "NL", "IT"]
)

# ---------- F√∂rb√§ttrade hj√§lpfunktioner ----------
@st.cache_data(ttl=3600)  # Cache i 1 timme
def fetch_author_details(author_id: str) -> Dict:
    """H√§mta detaljerad information om en f√∂rfattare"""
    try:
        clean_id = author_id.split('/')[-1] if 'openalex.org' in author_id else author_id
        response = requests.get(f"https://api.openalex.org/authors/{clean_id}", timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        st.warning(f"Kunde inte h√§mta data f√∂r f√∂rfattare {author_id}: {e}")
        return {}

@st.cache_data(ttl=3600)
def get_author_research_profile(author_id: str, num_works: int = 5) -> str:
    """H√§mta f√∂rfattarens forsknungsprofil baserad p√• senaste publikationer"""
    try:
        clean_id = author_id.split('/')[-1] if 'openalex.org' in author_id else author_id
        works_url = f"https://api.openalex.org/works?filter=author.id:{clean_id}&sort=publication_year:desc&per-page={num_works}"
        
        response = requests.get(works_url, timeout=10)
        response.raise_for_status()
        works = response.json().get("results", [])
        
        # Kombinera abstracts, titlar och concepts
        research_texts = []
        for work in works:
            text_parts = []
            
            # L√§gg till titel
            if work.get("title"):
                text_parts.append(work["title"])
            
            # L√§gg till abstract om det finns
            if work.get("abstract_inverted_index"):
                # Rekonstruera abstract fr√•n inverterat index
                abstract = reconstruct_abstract_from_inverted_index(work["abstract_inverted_index"])
                if abstract:
                    text_parts.append(abstract)
            
            # L√§gg till concepts
            concepts = [c.get("display_name", "") for c in work.get("concepts", [])[:5]]
            if concepts:
                text_parts.extend(concepts)
            
            if text_parts:
                research_texts.append(" ".join(text_parts))
        
        return " ".join(research_texts) if research_texts else ""
        
    except requests.RequestException as e:
        st.warning(f"Kunde inte h√§mta publikationer f√∂r {author_id}: {e}")
        return ""

def reconstruct_abstract_from_inverted_index(inverted_index: Dict) -> str:
    """Rekonstruera abstract fr√•n OpenAlex inverterat index"""
    try:
        # Skapa en lista med r√§tt l√§ngd
        word_positions = []
        for word, positions in inverted_index.items():
            for pos in positions:
                word_positions.append((pos, word))
        
        # Sortera efter position och skapa text
        word_positions.sort(key=lambda x: x[0])
        abstract = " ".join([word for pos, word in word_positions])
        
        # Begr√§nsa l√§ngd f√∂r prestanda
        return abstract[:2000] if len(abstract) > 2000 else abstract
    except:
        return ""

def is_active_recently(author_json: Dict, years: int = 5) -> bool:
    """Kontrollera om f√∂rfattare varit aktiv nyligen"""
    current_year = datetime.datetime.now().year
    counts = author_json.get("counts_by_year", [])
    recent = [c for c in counts if c["year"] >= current_year - years + 1]
    total_recent_pubs = sum(c["works_count"] for c in recent)
    return total_recent_pubs > 0

def find_authors_by_concept(concept_id: str, per_page: int = 20) -> List[Dict]:
    """Hitta f√∂rfattare baserat p√• concept ID"""
    try:
        url = f"https://api.openalex.org/authors?filter=concepts.id:{concept_id}&sort=cited_by_count:desc&per-page={per_page}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json().get("results", [])
    except requests.RequestException:
        return []

def find_authors_by_keyword(keyword: str, per_page: int = 20) -> List[Dict]:
    """Hitta f√∂rfattare baserat p√• keyword-s√∂kning i publikationer"""
    try:
        url = f"https://api.openalex.org/works?search={keyword}&sort=cited_by_count:desc&per-page={per_page}"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        works = response.json().get("results", [])
        
        authors = []
        seen_authors = set()
        
        for work in works:
            for authorship in work.get("authorships", []):
                author = authorship.get("author", {})
                author_id = author.get("id", "")
                
                if author_id and author_id not in seen_authors:
                    seen_authors.add(author_id)
                    authors.append({
                        "id": author_id,
                        "display_name": author.get("display_name", "Ok√§nd"),
                        "matched_term": keyword,
                        "work_title": work.get("title", ""),
                        "cited_by_count": work.get("cited_by_count", 0)
                    })
                    
                    if len(authors) >= per_page:
                        break
            if len(authors) >= per_page:
                break
                
        return authors
    except requests.RequestException:
        return []

def calculate_weighted_score(author_data: Dict, call_text: str, selected_keywords: List[str], 
                           call_embedding: np.ndarray, max_citations: int) -> float:
    """Ber√§kna viktad matchningspo√§ng f√∂r en f√∂rfattare"""
    
    # 1. Semantisk likhet baserad p√• forskningsprofil
    research_profile = author_data.get('research_profile', '')
    if research_profile.strip():
        try:
            author_embedding = embed_model.encode(research_profile, convert_to_tensor=True, device="cpu")
            semantic_score = util.cos_sim(call_embedding, author_embedding)[0][0].item()
        except:
            semantic_score = 0.0
    else:
        semantic_score = 0.0
    
    # 2. Keyword-matchningspo√§ng
    matched_terms = set(term.lower() for term in author_data.get('matched_terms', []))
    selected_keywords_lower = set(kw.lower() for kw in selected_keywords)
    keyword_matches = len(matched_terms.intersection(selected_keywords_lower))
    keyword_score = min(keyword_matches / len(selected_keywords), 1.0) if selected_keywords else 0.0
    
    # 3. Impakt-po√§ng (normaliserad)
    citations = author_data.get('citations', 0)
    impact_score = min(citations / max(max_citations, 1), 1.0) if max_citations > 0 else 0.0
    
    # Viktad kombination
    final_score = (
        semantic_weight * semantic_score + 
        keyword_weight * keyword_score + 
        impact_weight * impact_score
    )
    
    return final_score, semantic_score, keyword_score, impact_score

# ---------- F√∂rb√§ttrad keyword-extraktion ----------
def extract_enhanced_keywords(text: str, num_keywords: int, ngram_range: int) -> List[str]:
    """F√∂rb√§ttrad keyword-extraktion med synonymer och dom√§nspecifika termer"""
    
    # Grundl√§ggande KeyBERT-extraktion
    keywords = kw_model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, ngram_range),
        stop_words="english",
        top_n=num_keywords * 2,  # Ta ut fler f√∂r att sedan filtrera
        use_mmr=True,  # Anv√§nd Maximal Marginal Relevance f√∂r m√•ngfald
        diversity=0.5
    )
    
    # Filtrera bort f√∂r generiska termer
    generic_terms = {
        'research', 'study', 'analysis', 'method', 'approach', 'development', 
        'investigation', 'application', 'project', 'work', 'data', 'results',
        'forskning', 'studie', 'analys', 'metod', 'utveckling', 'till√§mpning',
        'projekt', 'arbete', 'data', 'resultat'
    }
    
    filtered_keywords = []
    for keyword, score in keywords:
        if (len(keyword) > 2 and 
            keyword.lower() not in generic_terms and 
            not keyword.replace(' ', '').isdigit()):
            filtered_keywords.append(keyword)
            
        if len(filtered_keywords) >= num_keywords:
            break
    
    return filtered_keywords

# ---------- Steg 1: F√∂rb√§ttrad s√∂kord-extraktion ----------
if st.button("üîë Extrahera s√∂kord (F√∂rb√§ttrad)") or 'extracted_keywords' in st.session_state:
    if call_text.strip():
        if 'extracted_keywords' not in st.session_state:
            with st.spinner("Extraherar relevanta s√∂kord..."):
                keywords = extract_enhanced_keywords(call_text, num_keywords, ngram_range)
                # L√§gg till egna s√∂kord f√∂rst (om de inte redan finns)
                for custom_kw in custom_keywords:
                    if custom_kw and custom_kw not in keywords:
                        keywords.insert(0, custom_kw)
                st.session_state['extracted_keywords'] = keywords
                st.session_state['selected_keywords'] = keywords.copy()

        # Visa checkboxar
        st.write("### Identifierade s√∂kord/fraser (v√§lj de som √§r relevanta)")
        col1, col2 = st.columns(2)
        for i, kw in enumerate(st.session_state['extracted_keywords']):
            col = col1 if i % 2 == 0 else col2
            with col:
                checked = kw in st.session_state['selected_keywords']
                # Visa ikon f√∂r egna s√∂kord
                icon = "üõ†Ô∏è " if custom_keywords and kw in custom_keywords else "üîç "
                val = st.checkbox(f"{icon}{kw}", value=checked, key=f"chk_{kw}")
                if val and kw not in st.session_state['selected_keywords']:
                    st.session_state['selected_keywords'].append(kw)
                elif not val and kw in st.session_state['selected_keywords']:
                    st.session_state['selected_keywords'].remove(kw)
    else:
        st.warning("Klistra in utlysningstext f√∂rst.")

# ---------- Steg 2: F√∂rb√§ttrad forskarh√§mtning ----------
if st.button("‚úÖ H√§mta forskare (F√∂rb√§ttrad matchning)"):
    selected_keywords = st.session_state.get('selected_keywords', [])
    if not selected_keywords:
        st.warning("V√§lj minst ett s√∂kord fr√•n steg 1.")
    else:
        with st.spinner("H√§mtar forskare med f√∂rb√§ttrad matchning..."):
            
            # F√∂rencode utlysningstext f√∂r semantisk matchning
            call_embedding = embed_model.encode(call_text, convert_to_tensor=True, device="cpu")
            
            # Progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Hitta concepts
            status_text.text("S√∂ker forskningsconcept...")
            concepts = []
            for i, kw in enumerate(selected_keywords):
                try:
                    response = requests.get(f"https://api.openalex.org/concepts?search={kw}")
                    results = response.json().get("results", [])
                    if results:
                        concepts.append(results[0])
                except:
                    continue
                progress_bar.progress((i + 1) / (len(selected_keywords) * 2))
            
            # H√§mta f√∂rfattare
            temp_authors = {}
            
            # Fr√•n concepts
            status_text.text("H√§mtar forskare fr√•n concepts...")
            for i, concept in enumerate(concepts):
                authors = find_authors_by_concept(concept["id"], per_page=num_per_source)
                for author in authors:
                    author_id = author.get("id", "")
                    if author_id and author_id not in temp_authors:
                        temp_authors[author_id] = {
                            "matched_terms": set([concept.get("display_name", "")]),
                            "author_obj": author
                        }
                    elif author_id:
                        temp_authors[author_id]["matched_terms"].add(concept.get("display_name", ""))
                
                progress_bar.progress((len(selected_keywords) + i + 1) / (len(selected_keywords) * 2))
            
            # Fr√•n keywords
            for i, keyword in enumerate(selected_keywords):
                authors = find_authors_by_keyword(keyword, per_page=num_per_source)
                for author in authors:
                    author_id = author.get("id", "")
                    if author_id and author_id not in temp_authors:
                        temp_authors[author_id] = {
                            "matched_terms": set([keyword]),
                            "author_obj": author
                        }
                    elif author_id:
                        temp_authors[author_id]["matched_terms"].add(keyword)
            
            progress_bar.progress(1.0)
            status_text.text("Analyserar forskarprofiler...")
            
            # H√§mta detaljerade profiler och ber√§kna po√§ng
            author_list = []
            max_citations = 0
            
            # F√∂rsta g√•ngen igenom - hitta max citeringar f√∂r normalisering
            for author_id, info in temp_authors.items():
                details = fetch_author_details(author_id)
                if details:
                    citations = details.get("cited_by_count", 0)
                    max_citations = max(max_citations, citations)
            
            for i, (author_id, info) in enumerate(temp_authors.items()):
                details = fetch_author_details(author_id)
                
                if not details or not is_active_recently(details, years=5):
                    continue
                
                # H√§mta forskningsprofil
                research_profile = get_author_research_profile(author_id)
                
                institutions = details.get("last_known_institutions", [])
                inst_name = institutions[0].get("display_name", "Ok√§nd institution") if institutions else "Ok√§nd institution"
                country = institutions[0].get("country_code", "Ok√§nt land") if institutions else "Ok√§nt land"
                
                works = details.get("works_count", 0)
                citations = details.get("cited_by_count", 0)
                
                # Skapa f√∂rfattardata f√∂r po√§ngber√§kning
                author_data = {
                    'research_profile': research_profile,
                    'matched_terms': list(info["matched_terms"]),
                    'citations': citations
                }
                
                # Ber√§kna viktad po√§ng
                final_score, semantic_score, keyword_score, impact_score = calculate_weighted_score(
                    author_data, call_text, selected_keywords, call_embedding, max_citations
                )
                
                author_list.append({
                    "Namn": details.get("display_name", "Ok√§nd"),
                    "Institution": inst_name,
                    "Land": country,
                    "Publikationer": works,
                    "Citeringar": citations,
                    "Profil": author_id,
                    "Matched_terms": ", ".join(info["matched_terms"]),
                    "Viktad_po√§ng": round(final_score, 3),
                    "Semantisk_po√§ng": round(semantic_score, 3),
                    "S√∂kord_po√§ng": round(keyword_score, 3),
                    "Impakt_po√§ng": round(impact_score, 3),
                    "Forskningsprofil": research_profile[:200] + "..." if len(research_profile) > 200 else research_profile
                })
                
                # Uppdatera progress
                if i % 10 == 0:
                    status_text.text(f"Analyserat {i+1}/{len(temp_authors)} forskare...")
            
            progress_bar.empty()
            status_text.empty()
            
            df = pd.DataFrame(author_list)
            
            if df.empty:
                st.warning("Inga forskare hittades med dessa kriterier.")
            else:
                # Landfilter
                if include_countries:
                    df = df[df["Land"].isin(include_countries)]
                if exclude_countries:
                    df = df[~df["Land"].isin(exclude_countries)]
                
                if df.empty:
                    st.warning("Inga forskare kvar efter landfiltrering.")
                else:
                    # Sortera efter viktad po√§ng
                    df = df.sort_values("Viktad_po√§ng", ascending=False)
                    
                    # Visa statistik
                    st.success(f"Hittade {len(df)} aktiva forskare!")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Medel semantisk po√§ng", f"{df['Semantisk_po√§ng'].mean():.3f}")
                    with col2:
                        st.metric("Medel s√∂kord-po√§ng", f"{df['S√∂kord_po√§ng'].mean():.3f}")
                    with col3:
                        st.metric("Medel impakt-po√§ng", f"{df['Impakt_po√§ng'].mean():.3f}")
                    
                    # Visa tabell med f√∂rb√§ttrade kolumner
                    display_df = df[[
                        "Namn", "Institution", "Land", "Publikationer", "Citeringar",
                        "Viktad_po√§ng", "Semantisk_po√§ng", "S√∂kord_po√§ng", "Impakt_po√§ng",
                        "Matched_terms", "Profil"
                    ]].copy()
                    
                    st.data_editor(
                        display_df,
                        column_config={
                            "Profil": st.column_config.LinkColumn("Profil", display_text="√ñppna profil"),
                            "Viktad_po√§ng": st.column_config.NumberColumn("Viktad po√§ng", format="%.3f"),
                            "Semantisk_po√§ng": st.column_config.NumberColumn("Semantisk", format="%.3f"),
                            "S√∂kord_po√§ng": st.column_config.NumberColumn("S√∂kord", format="%.3f"),
                            "Impakt_po√§ng": st.column_config.NumberColumn("Impakt", format="%.3f"),
                        },
                        hide_index=True,
                        use_container_width=True
                    )
                    
                    # Visa forskningsprofiler f√∂r topp 5
                    st.subheader("üß¨ Forskningsprofiler (Topp 5)")
                    for i, (_, author) in enumerate(df.head(5).iterrows()):
                        with st.expander(f"{i+1}. {author['Namn']} - {author['Institution']}"):
                            st.write(f"**Viktad po√§ng:** {author['Viktad_po√§ng']}")
                            st.write(f"**Matchade termer:** {author['Matched_terms']}")
                            st.write(f"**Forskningsprofil:**")
                            st.write(author['Forskningsprofil'])

# ---------- Information om f√∂rb√§ttringar ----------
with st.expander("‚ÑπÔ∏è Vad √§r nytt i denna version?"):
    st.markdown("""
    ### üöÄ F√∂rb√§ttringar f√∂r b√§ttre matchningar:
    
    **1. Abstract-baserad semantisk matchning**
    - Analyserar forskares faktiska publikationer och abstracts
    - Mycket mer exakt matchning √§n bara namn + institution
    
    **2. Viktad scoring**
    - Kombinerar semantisk likhet, keyword-matchningar och forskningsimpakt
    - Anpassningsbara vikter i sidopanelen
    
    **3. F√∂rb√§ttrad keyword-extraktion**
    - Anv√§nder MMR (Maximal Marginal Relevance) f√∂r mer varierade s√∂kord
    - Filtrerar bort generiska termer
    
    **4. Prestanda-optimeringar**
    - Caching av API-anrop
    - Progress bars f√∂r anv√§ndarfeedback
    - B√§ttre felhantering
    
    **5. Ut√∂kad analys**
    - Visar forskningsprofiler f√∂r toppresultat
    - Separata po√§ng f√∂r olika matchningstyper
    """)







